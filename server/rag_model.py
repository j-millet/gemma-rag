import torch
from transformers import AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig
import colorama
import gc

class rag_model:
    def __init__(self,model_name,quantization_config = None) -> None:
        self.special_tokens = ['<bos>','<eos>','<pad>','<unk>','<start_of_turn>','<end_of_turn>']

        print(colorama.Fore.YELLOW + f"Loading {model_name}...")
        print(colorama.Style.RESET_ALL,end="")

        if (not torch.cuda.is_available()) and (quantization_config is None):
            print(colorama.Fore.RED + "CUDA not available. Default quantization will not work...")
            print(colorama.Style.RESET_ALL,end="")
            exit()

        if quantization_config is None:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16
            )
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                device_map="cuda" if torch.cuda.is_available() else "auto",
                quantization_config=quantization_config
            )
            print(colorama.Fore.GREEN + "Model loaded.")
            print(colorama.Style.RESET_ALL,end="")
        except Exception as e:
            print(colorama.Fore.RED + f"Jesus fucking christ what is happening: {e}")
            print(colorama.Style.RESET_ALL,end="")
            torch.cuda.empty_cache()

    def unload(self):
        '''
        Wipes model from GPU memory
        '''
        del self.model, self.tokenizer
        torch.cuda.empty_cache()
        gc.collect()

    def make_prediction(self,chat_history,incomplete_message=False,max_new_tokens=10,temperature=0.1):
        '''
        Generate model predictions based on chat history.

        Args: 
            chat_history: list of alternating (user-assistant) messages ({'role':<role>, 'content':<content>})
            incomplete_message: If set to True we assume that the model hasn't finished generating a response. (text input for the model won't have an <end_of_turn> tag)
            max_new_tokens: max number of tokens generated by model
            temperature: The higher the temperature the less probable tokens the model will choose (higher temp = higher 'creativity')
        '''
        input_text = ""
        if (not incomplete_message) or chat_history[-1]["role"] == "user":
            input_text = self.tokenizer.apply_chat_template(chat_history,tokenize=False,add_generation_prompt=True)
        else:
            input_text = self.tokenizer.apply_chat_template(chat_history[:-1],tokenize=False,add_generation_prompt=False)
            input_text += f"<start_of_turn>model\n{chat_history[-1]['content']}"

        input_ids = self.tokenizer(input_text, add_special_tokens=False,return_tensors="pt").to("cuda")

        outputs = self.model.generate(**input_ids,max_new_tokens=max_new_tokens,temperature=temperature)
        text = self.tokenizer.decode(outputs[0])

        model_message = text.replace(input_text,"")
        incomplete_message = not "<eos>" in model_message
        for special_token in self.special_tokens:
            model_message = model_message.replace(special_token,"")

        return model_message,incomplete_message

        